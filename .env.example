# ======================================
# Agent (AGENT_*) - Browser automation
# ======================================

# AWS Bedrock regions (comma-separated, preference order)
AGENT_AWS_REGION=ap-northeast-1,us-west-2

# Model IDs (either plural or single). Example model ID shown below is placeholder
# Use either AGENT_BEDROCK_MODEL_IDS or AGENT_BEDROCK_MODEL_ID
AGENT_BEDROCK_MODEL_IDS=global.anthropic.claude-sonnet-4-20250514-v1:0
AGENT_BEDROCK_MODEL_ID=

# Rerank settings (region and optional model ARN override)
AGENT_BEDROCK_RERANK_REGION=us-west-2
AGENT_BEDROCK_RERANK_MODEL_ARN=

# Path to CSV produced by the crawler (defaults to output/crawl.csv when unset)
AGENT_CSV_PATH=

# Headful browser: true/false (default false)
AGENT_HEADFUL=false

# Optional pre-login domain and storage state file
AGENT_BROWSER_DOMAIN=https://example.com/
AGENT_STORAGE_STATE_FILE=

# Optional agent prompt used when CLI --prompt is omitted
AGENT_QUERY=

# Playwright timeout (ms)
AGENT_PLAYWRIGHT_TIMEOUT_MS=5000

# Snapshot chunking and search results
AGENT_SNAPSHOT_MAX_CHUNK_SIZE=1500
AGENT_SNAPSHOT_MIN_CHUNK_SIZE=500
AGENT_BROWSER_TOP_K=3
AGENT_SEARCH_TOP_K=5

# Extended/Interleaved Thinking (supported models only)
AGENT_THINKING_ENABLED=false
AGENT_THINKING_BUDGET_TOKENS=1024
AGENT_THINKING_INTERLEAVED=false
AGENT_MAX_TOKENS=4096

# Optional browser login credentials used by tools/browser-login.ts
AGENT_BROWSER_USERNAME=
AGENT_BROWSER_PASSWORD=

# Optional browser close delay (ms) after finishing
AGENT_BROWSER_CLOSE_DELAY_MS=5000

# Optional Anthropic beta header value for special features
AGENT_ANTHROPIC_BETA=

# ======================================
# Observability (Langfuse) - Optional
# ======================================

LANGFUSE_PUBLIC_KEY=
LANGFUSE_SECRET_KEY=
LANGFUSE_HOST=

# Optional custom cost settings (USD per token). Leave empty to use defaults
AGENT_LANGFUSE_COST_INPUT_PER_TOKEN=
AGENT_LANGFUSE_COST_OUTPUT_PER_TOKEN=
AGENT_LANGFUSE_COST_CACHE_READ_INPUT_PER_TOKEN=
AGENT_LANGFUSE_COST_CACHE_WRITE_INPUT_PER_TOKEN=

# ======================================
# Crawler (CRAWLER_*) - Pre-crawl â†’ CSV
# ======================================

# Seeds (comma/whitespace separated). Defaults to demo when unset
CRAWLER_TARGET_URLS=https://books.toscrape.com/

# Optional login target and credentials
CRAWLER_LOGIN_URL=
CRAWLER_LOGIN_USER=
CRAWLER_LOGIN_PASS=

# Enable/disable login attempts (default true)
CRAWLER_AUTH_ENABLED=true

# Headful/browser visibility and CSV behavior
CRAWLER_HEADFUL=false
CRAWLER_CLEAR_CSV=false

# Timeouts and limits
CRAWLER_PLAYWRIGHT_TIMEOUT_MS=5000
CRAWLER_MAX_URLS=

# Output CSV (takes precedence) or alternate CSV path env
CRAWLER_OUTPUT_FILE=output/crawl.csv
CRAWLER_CSV_PATH=

# Misc crawler options
CRAWLER_DEDUPE_ELEMENTS_PER_BASE=false
CRAWLER_STORAGE_STATE_FILE=

# ======================================
# Global / Debug - Optional
# ======================================

# Global Playwright timeout (ms). If set, applies unless AGENT_/CRAWLER_ overrides
# PLAYWRIGHT_TIMEOUT_MS=

# Print extracted internal URLs during snapshot parsing
# DEBUG_SNAPSHOT_URLS=false

# Cache debugging toggles
# AGENT_DEBUG_CACHEPOINTS=false
# AGENT_DEBUG_ELIDE_SNAPSHOTS=false

# X display variable (Linux). Headful requires DISPLAY; leave unset in headless
# DISPLAY=

# ======================================
# AWS Credentials (Do NOT commit real secrets)
# ======================================

# Preferred: use a named profile on the host machine
AWS_PROFILE=default

# Or set access keys for CI/local only (never commit real values)
# AWS_ACCESS_KEY_ID=
# AWS_SECRET_ACCESS_KEY=
# AWS_SESSION_TOKEN=

# ======================================
# E2E / CI - Optional
# ======================================

# Test-only knobs used by e2e scripts
# E2E_CSV_PATH=
# E2E_REUSE_EXISTING_CSV=false


